# Information about the code

## Folder contents
This folder contains 5 files and two sub-folders.

The files in this folder are:
* get_pmids.sh
* download_data.sh
* extract_data.sh
* clean_data.R
* visualise_data.R

The sub-folder setup contains the following files:
* hpc_setup_job.sh
* slurm_config.yaml
The sub-folder utils contains the following file:
* progress_bar.sh


## File contents
### setup
#### hpc_setup_job.sh
This file is a shell script to be submitted as a job to Slurm. This script is part of the HPC set-up process and it's purpose is to create a Slurm profile.

#### slurm_config.yaml
This file is a yaml file which provides the config for the Slurm profile.

### utils
#### progress_bar.sh
This file is a shell script which defines a function to produce a progress bar for downloading the article data. The progress bar appears in the terminal like 'Progress: [###-------] 30.00%'.

### get_pmids.sh
This file is a shell script to download the PubMed article IDs (PMID) for long covid articles.
The shell script downloads an XML file and then extracts the PMIDs and saves them as a single column tab-seperated values file (TSV) in ../data/raw/pmids.tsv

### download_data.sh
This file is a shell script to download the PubMed long covid articles using the E-utilities API and the pmids.tsv file generated by the get_pmids.sh file.
The shell script loads 3 articles in parallel (as a batch) and has a 1 second pause between each batch download. Only 3 articles can be downloaded at a time as the E-utilities API has a limit of 3 requests per second.
The articles are then saved as XML files in ../data/raw/article-data-{pmid}.xml

### extract_data.sh
This is a shell script to extract the data of interest from the PubMed article XML files in ../data/raw/article-data-{pmid}.xml
The shell script uses xmllint and sed to extract the values from the following XML tags:
* PMID = \<PMID\>
* Year = \<PubDate\>\<Year\>
* Article Title = \<ArticleTitle\>
* Abstract = \<AbstractText\>
PMID is the PubMed ID, Year is the year the article is published, Article Title is the article's title, and Abstract is the article's abstract.
The shell script extracts the data from every article in parallel and saves it as a TSV file in ../data/clean/extracted_data.tsv

### clean_data.R
This is an R script to clean the extracted data from the extract_data.sh output.
The R script loads the extracted data TSV file from ../data/clean/extracted_data.tsv and removes rows with missing values, articles with a publish date in 2020 or 2025, and removes non-alphabetical characters from the text data using regex.
Then the cleaned data are tidied so it is ready for analysis. The data are tidied using R's tidytext package, by unnesting tokens and removing stopwords, including subject specific words based off term frequency - inverse document frequency (tf-idf) analysis.
The R script then outputs the cleaned and tidied data to ../data/clean/{article_characteristic}_data.tsv.

### visualise_data.R
This is an R script to visualise the data analysis of the text data.
The R script loads the user specified cleaned and tidied data from ../data/clean/{article_characteristic}_data.tsv (the user can choose between 'title' and 'abstract' for the article characteristic) and then performs Latent Dirichlet Allocation (LDA) topic modelling with k (user specified) topics. The R script then outputs a plot showing the 5 most common terms for each topic as ../results/article_{article_characteristic}_topic_terms.png so the user can interpret the topics, and a plot showing the change in the proportion of topics over time as ../results/article_{article_characteristic}_topics_over_time.png.
